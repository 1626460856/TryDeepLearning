loss = nn.CrossEntropyLoss()
输出层是预测出的数据概率分布张量，每一个概率数据的范围是[0,1]，当然也可能是任意实数，比如负数
先把这个数据放入softmax函数，即能增强数值对于概率的描述，也能保证非负性
即对于每一个数据的单个原始概率：
关于计算方式呢大的框架是参考古典概率的计算，就是简单的p[i]=z[i]/(z[0]+z[1]+z[2]+...+z[i]+...+z[k])
为了增大具体值对概率的影响，并且达到一个非负的效果，将原先的数值每一个进行指数运算转化再带入求概率，即
z[i]=e^(z[i])
p[i]=z[i]/(z[0]+z[1]+z[2]+...+z[i]+...+z[k])
最后，我们的模型会输出一个概率分布，代表着对于给定输入图像，模型认为每个类别的可能性有多大。
我们可以根据这个概率分布来做出最终的分类决策。
选择具有最高概率的类别作为最终的预测结果。这个步骤通常称为“argmax”操作，即取概率分布中概率最大的位置所对应的类别作为预测结果。
y_hat = y_hat.argmax(axis=1)
对于一批数据，如256个数据为一组数据
计算出分类正确的概率p=分类正确数据/总样本数据
由于p是属于[0,1]的数据，为了增大这个概率描述的损失，采用y=-ln(p)进行转化
当p->0时，y->正无穷
当p->1时，y->0
最后按照y描述出的损失，修改权重偏置等等
